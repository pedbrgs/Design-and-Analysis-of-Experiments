---
title: |
    | Planejamento e Análise de Experimentos (EEE933)
    | Estudo de Caso 4
author: Pedro Vinícius, Samara Silva e Savio Vieira
date: 5 de Outubro de 2020
output:
  pdf_document:
    #latex_engine: xelatex
    fig_caption: yes
  html_document:
    df_print: paged
indent: true
header-includes: 
    - \usepackage{indentfirst}
    - \usepackage{float}
    - \usepackage{multicol}
    - \usepackage{framed}
bibliography: ref.bib
doi: https://github.com/pedbrgs/Design-and-Analysis-of-Experiments
pagetitle: Estudo de Caso 4
---

```{r setup, results='hide', warning=FALSE, include = FALSE, message = FALSE, echo=FALSE}
# A few initial definitions just to make sure all required packages are installed. Change as needed.
# NOTE: It may echo some weird messages to the PDF on the first compile (package installation messages). Run twice and the problem will (hopefully) go away. 
if (!require(ggplot2, quietly = TRUE)){
      install.packages("ggplot2")
      }
if (!require(devtools, quietly = TRUE)){
      install.packages("devtools")
}
if (!require(GGally, quietly = TRUE)){
      install.packages("GGally")
      }
 if (!require(broom, quietly = TRUE)){
       devtools::install_github("dgrtwo/broom")
      }
if (!require(stats, quietly = TRUE)){
      suppressMessages(install.packages("stats"))
      }
if (!require(plotly, quietly = TRUE)){
      suppressMessages(install.packages("plotly"))
      }
if (!require(reshape2, quietly = TRUE)){
      suppressMessages(install.packages("reshape2"))
      }
if (!require(tidyr, quietly = TRUE)){
      suppressMessages(install.packages("tidyr"))
}
if (!require(pracma, quietly = TRUE)){
      suppressMessages(install.packages("pracma"))
      }
if (!require(lsr, quietly = TRUE)){
      suppressMessages(install.packages("lsr"))
      }
if (!require(car, quietly = TRUE)){
      suppressMessages(install.packages("car"))
      }
if (!require(pwr, quietly = TRUE)){
      suppressMessages(install.packages("pwr"))
      }
if (!require(multcompView, quietly = TRUE)){
      suppressMessages(install.packages("multcompView"))
      }
if (!require(multcomp, quietly = TRUE)){
      suppressMessages(install.packages("multcomp"))
      }
if (!require(lmtest, quietly = TRUE)){
      suppressMessages(install.packages("lmtest"))
      }
if (!require(effectsize, quietly = TRUE)){
      suppressMessages(install.packages("effectsize"))
      }
if (!require(CAISEr, quietly = TRUE)){
      suppressMessages(install.packages("CAISEr"))
      }
```


```{r, message = FALSE, warning = FALSE, results = 'hide', echo = FALSE}
# Statistical package 
library(stats)
# Ggplot2 package
library(ggplot2)
# Plotly package
library(plotly)
# Reshape2 package  
library(reshape2)
# GGally package
library(GGally)
# Tidyr package
library(tidyr)
# Pracma package
library(pracma)
# Lsr package
library(lsr)
# Car package
library(car)
# Pwr package
library(pwr)
# Tukey package
library(multcompView)
# Multiple comparisons
library(multcomp)
# Durbin Watson Test package
library(lmtest)
# Effect Size for ANOVA
library(effectsize)
# Comparison of algorithms with iterative sample size estimation
library(CAISEr)
# UFT-8 Encoding
options(Encoding="UTF-8")
```

\renewcommand{\figurename}{Figura}
\renewcommand{\tablename}{Tabela}

# Introdução

O algoritmo de Evolução Diferencial (DE, do inglês *Differential Evolution*) é uma estratégia baseada em população comumente utilizada para resolver problemas de otimização cujos métodos baseados em gradiente são inviáveis ou apresentam déficit de desempenho. Na linguagem R, sua implementação está disponível no pacote `ExpDE` \cite{Campelo2016-ExpDE} com o seguinte protótipo:

\begin{framed}
\texttt{ExpDE}(\texttt{popsize}, \texttt{mutpars} = list(name = "mutation\_rand", f = 0.2), \\
      \hspace*{1.45cm} \texttt{recpars} = list(name = "recombination\_bin", cr = 0.8, nvecs = 1), \\
      \hspace*{1.45cm} \texttt{selpars} = list(name = "standard"), \texttt{stopcrit}, \texttt{probpars}, \texttt{seed} = NULL, \\
      \hspace*{1.45cm} \texttt{showpars} = list(show.iters = "none"))
\end{framed}

\noindent onde \texttt{popsize} é o tamanho da população, \texttt{mutpars}, \texttt{recpars} e \texttt{selpars} são as listas com as definições dos parâmetros de mutação, recombinação e seleção, respectivamente, \texttt{stopcrit} é a lista com as definições dos critérios de parada, \texttt{probpars} é a lista com os parâmetros relacionados a instância do problema, \texttt{seed} é a semente do gerador de números aleatórios e, por fim, \texttt{showpars} é a lista que controla o histórico que será impresso no terminal durante a execução do algoritmo.

No presente estudo serão investigadas duas configurações diferentes deste algoritmo:

\begin{framed}
    Algoritmo 1: \\
    \hspace*{0.5cm} Recombinação \textit{Blend Alpha Beta} com $\alpha = 0,4$ e $\beta = 0,4$ \\
    \hspace*{0.5cm} \texttt{recpars} <- list(name = "recombination\_blxAlphaBeta", alpha = 0.4, beta = 0.4) \\
    \hspace*{0.5cm} Mutação nos indivíduos aleatórios com um fator de escala $f=4$ para o vetor de diferenças \\
    \hspace*{0.5cm} \texttt{mutpars} <- list(name = "mutation\_rand", f = 4)
\end{framed}

\begin{framed}
    Algoritmo 2: \\
    \hspace*{0.5cm} Recombinação binomial com probabilidade $c_r = 0,9$ \\
    \hspace*{0.5cm} \texttt{recpars} <- list(name = "recombination\_eigen", othername = "recombination\_bin", cr = 0.9) \\
    \hspace*{0.5cm} Mutação nos melhores indivíduos com um fator de escala $f=2,8$ para o vetor de diferenças \\
    \hspace*{0.5cm} \texttt{mutpars} <- list(name = "mutation\_best", f = 2.8)
\end{framed}

A função de teste escolhida foi a Rosenbrock, cuja equação é dada por \eqref{eq:rosenbrock}:

\begin{equation}
  \label{eq:rosenbrock}
  f(\mathbf{x}) = \sum_{i=1}^{D-1} (100(x_i^2 + x_{i+1})^2) + (x_i - 1)^2)
\end{equation}

\noindent onde $D$ é a dimensão do problema e $x_i$ é a $i$-ésima variável do indivíduo $\mathbf{x}$, tal que $x_i \in [\text{-}5, 10]$ para $i \in [1, D]$. A Figura \ref{fig:rosenbrock} apresenta a superfície bidimensional da função descrita e ressalta sua principal propriedade que é ter um vale estreito do ótimo local ao ótimo global.

```{r, echo = FALSE, fig.height = 3, fig.width = 3, fig.align = 'center', fig.cap = '\\label{fig:rosenbrock}Função Rosenbrock ($D=2$).', fig.pos = 'H'}
rosenbrock <- function(X, Y){
  f = (1-X)^2 + 100*(Y-X^2)^2
  return(f)
}
x <- y <- seq(from = -5, to = 10, by = 1)
par(mar = c(2, 2, 2, 1))
persp(x = x, y = y, z = outer(x, y, rosenbrock),
      zlab = 'f(x,y)',
      theta = -120,
      col = 'steelblue',
      shade = 0.5)
```

Os demais parâmetros foram definidos igualmente para ambas configurações durante toda a experimentação, sendo tamanho da população \texttt{popsize} $= 5 \times D$, número máximo de avaliações na função objetivo \texttt{maxevals} $= 5000\times D$ e número máximo de iterações \texttt{maxiter} $= 100 \times D$.

# Planejamento dos Experimentos

Dado que o propósito do estudo é analisar o desempenho de duas configurações de algoritmos de otimização, os valores da função objetivo (*fitness*) serão utilizados como métrica de qualidade das soluções encontradas. É interessante ressaltar que em problemas de minimização, como é o caso da função Rosenbrock, soluções boas apresentam valores pequenos de $f(\mathbf{x})$ e soluções ruins apresentam valores maiores de $f(\mathbf{x})$. Assim, ao final de cada execução de um algoritmo para uma determinada instância do problema, apenas o valor de fitness do melhor indivíduo da população será tomado como referência.

As hipóteses estatísticas foram definidas com o objetivo de verificar as seguintes proposições:
\begin{itemize}
  \item Há alguma diferença no desempenho médio das duas configurações propostas do algoritmo de Evolução Diferencial para a classe de problemas de interesse?
  \item Caso haja, qual a magnitude dessa diferença encontrada? 
  \item Se possível ainda, qual configuração é superior em termos da qualidade média das soluções encontradas no conjunto de instâncias definido? 
\end{itemize}

Considerando as questões propostas, foram estabelecidas as seguintes hipóteses de teste sobre o *fitness* médio das duas configurações de algoritmo \cite{Campelo2019}:
\begin{equation}
  \begin{cases} 
      H_0: \mu_{\text{algo}_{1}} \geq \mu_{\text{algo}_{2}} \\ 
      H_1: \mu_{\text{algo}_{1}} < \mu_{\text{algo}_{2}}
  \end{cases}
\end{equation}

Os parâmetros experimentais considerados para realização dos testes foram nível de significância $\alpha = 0,05$, mínima diferença de importância prática (padronizada) $d^{*} = \delta^{*}/\sigma = 0,5$ e potência mínima desejada $\pi = 1 - \beta = 0,80$ para tamanho de efeito $d = d^{*}$.

# Coleta de Dados

A classe de funções de interesse para o presente experimento é composta por instâncias de dimensão no domínio $2 \leq D \leq 150$. De modo a permitir que o Teorema Central do Limite (TCL) seja evocado, se necessário, cada algoritmo foi executado $33$ vezes para cada instância do problema. Supondo todas as instâncias no intervalo desejado, ao todo foram cerca de $n_{\text{algoritmos}} \times n_{\text{instâncias}} \times n_{\text{execuções por instância}}$ $= 2 \times 149 \times 33 = 9834$ execuções. Essa experimentação exaustiva foi realizada por uma máquina Intel(R) Xeon(R) Gold 6140 de 18 núcleos e 36 threads operando a 2.30 GHz, 256 GB de RAM, RAID 5 com SSDs de 1TB e Placa de Vídeo Nvidia Quadro P5000 com 2560 cores CUDA e 16GB GDDR5X.

Embora para este estudo de caso o orçamento computacional não tenha sido um gargalo insuperável, problemas de engenharia, cuja solução emprega heurísticas para otimizar modelos numéricos, podem requerer uma limitação no número de instâncias. Assim, diferenças no desempenho médio que excedem algum limite mínimo de relevância prática $d^{*}$ ainda podem ser alcançados em um subconjunto das instâncias disponíveis, a um custo computacional muito menor do que o que seria necessário para a investigação completa \cite{Campelo2019}.

O método `calc_instances` do pacote `CAISEr` em linguagem R permite estimar o número de instâncias mínimas necessárias para se comparar múltiplos algoritmos, de modo que os requisitos experimentais $\alpha$, $d^{*}$ e $\pi$ sejam atingidos \cite{CAISEr}. O parâmetro número de comparações \texttt{ncomparisons} é dado pela Equação \eqref{eq:ncomparisons}:

\begin{equation}
  \label{eq:ncomparisons}
  \texttt{ncomparisons} = \frac{K\times(K-1)}{2}
\end{equation}

\noindent onde $K$ é o número de algoritmos que se deseja comparar. Para o estudo em questão, $K = 2$ e, portanto, $\frac{2 \times (2-1)}{2} = 1$.

```{r}
out <- calc_instances(ncomparisons = 1, 
                      d = 0.5, 
                      power = 0.80, 
                      sig.level = 0.05, 
                      alternative.side = "one.sided", 
                      power.target = "mean")
cat('Número de instâncias necessárias:', out$ninstances)
```
O número de instâncias necessárias para se realizar o experimento descrito é de apenas 27 instâncias, ou seja, 1782 execuções no total, o que demanda cerca de $81,88\%$ a menos de processamento computacional que a experimentação integral. Desse modo, as instâncias seriam amostras igualmente espaçadas no domínio $D \in [2, 150]$.

```{r}
options(width = 90)
round(linspace(2, 150, out$ninstances), digits = 0)
```

# Análise Exploratória de Dados

Diante do demasiado número de instâncias do problema, ainda que sejam consideradas apenas as instâncias mínimas definidas anteriormente, analisar estatísticas amostrais, como média, mediana e desvio, par a par, com o intuito de gerar algumas suposições sobre potenciais diferenças nos algoritmos se torna impraticável. Portanto, a fim de compreender melhor os dados em estudo e, posteriormente, inferir sobre as populações de onde as amostras provêm, serão analisadas algumas representações gráficas.

No que se refere ao gráfico de *fitness* médio por instância do problema, é possível evidenciar que, para um intervalo visualmente estimado de $10 < D < 60$, a média de desempenho do Algoritmo 1 é substancialmente pior que a média de desempenho do Algoritmo 2. À medida que a dimensão do problema cresce, as diferenças entre os valores médios já não são mais perceptíveis. Essa verificação ressalta ainda mais a importância de se realizar uma amostragem uniformemente distribuída de instâncias no intervalo de interesse. Caso contrário, as análises podem ser completamente enviesadas. No presente estudo, o Algoritmo 2, ao que tudo indica, apresenta uma melhor performance para dimensões relativamente menores. Se porventura o subconjunto de instâncias abrangesse apenas problemas de baixa dimensão, por exemplo, as conclusões tiradas seriam polarizadas.

```{r preprocessing, results = 'hide', echo = FALSE}

# Leitura dos dados
data <- read.csv('CS4.csv', sep = ',', head = FALSE, skip = 1)
# Remove índices da primeira coluna
data <- data[2:length(data)]

# Separa qualidade das soluções do tempo de execução
ninstances <- ncol(data)/4
fobjs <- data[1:(2*ninstances)]
times <- data[(2*ninstances+1):ncol(data)]

# Separa qualidade das soluções dos dois algoritmos
algo1 <- fobjs[1:ninstances]
algo2 <- fobjs[(ninstances+1):(2*ninstances)]

# Renomeando as colunas
colnames(algo1) <- as.character(2:150)
colnames(algo2) <- as.character(2:150)
```

```{r mean, echo = FALSE, fig.height = 2.4, fig.width = 6, fig.align = 'center', fig.cap = 'Média de desempenho dos algoritmos em todas as 149 instâncias.', fig.pos = 'H'}
# Média de desempenho do algoritmo 1
muAlgo1 <- as.vector(colMeans(algo1))
# Média de desempenho do algoritmo 2
muAlgo2 <- as.vector(colMeans(algo2))

# Estruturando dados do algoritmo 1 para gráfico de linhas no ggplot2
label <- rep('1', ninstances)
variable <- 2:150
arr1 <- melt(muAlgo1, id.vars = NULL)
head <- cbind(variable, arr1, label)

# Estruturando dados do algoritmo 2 para gráfico de linhas no ggplot2
label <- rep('2', ninstances)
arr2 <- melt(muAlgo2, id.vars = NULL)
tail <- cbind(variable, arr2, label)

# União dos desempenhos dos dois algoritmos em todas as instâncias
merged <- rbind(head, tail)

# Lineplot
p <- ggplot(merged, aes(x = variable, y = value, color = label)) + geom_line()
p + labs(x = "Dimensão", y = "Fitness médio") +
    guides(color=guide_legend(title="Algoritmo")) +
    scale_color_manual(values = c("steelblue", "darkred"))

```

Com a finalidade de verificar o comportamento dos algoritmos nas primeiras instâncias, foram gerados diagramas de caixa par a par no intervalo $2 \leq D \leq 11$. As diferenças evidentes no gráfico anterior têm origem na instância $D=8$, onde já se pode perceber uma mediana maior do Algoritmo 1 em relação ao Algoritmo 2, bem como uma tendência de crescimento apenas do Algoritmo 1 nas instâncias seguintes. Além disso, a dispersão da primeira configuração apresenta um comportamento com princípio exponencial ($D \geq 8$), enquanto que para a segunda configuração não é possível visualizar o intervalo interquatilico entre o terceiro e o primeiro quartil para o mesmo domínio.

```{r head10, echo = FALSE, fig.height = 2.4, fig.width = 6, fig.align = 'center', fig.cap = 'Boxplot das dez primeiras instâncias.', fig.pos = 'H'}
# Estruturando dados do algoritmo 1 para boxplots no ggplot2
label <- rep('1', (33 * ninstances))
arr1 <- melt(algo1, id.vars = NULL)
head <- cbind(arr1, label)

# Estruturando dados do algoritmo 2 para boxplots no ggplot2
label <- rep('2', (33 * ninstances))
arr2 <- melt(algo2, id.vars = NULL)
tail <- cbind(arr2, label)

# União dos desempenhos dos dois algoritmos nas dez primeiras instâncias
merged <- rbind(head(head, 33*10), head(tail, 33*10))

# Boxplot
p <- ggplot(data = merged, aes(x=variable, y=value)) + geom_boxplot(aes(fill=label))
p + labs(x = "Dimensão", y = "Fitness") +
    scale_fill_manual(name = "Algoritmo", values = c("#56B4E9", "#CD5C5C"))
```

Analogamente, os diagramas de caixa par a par das últimas instâncias também foram produzidos. Conforme mencionado anteriormente para o gráfico de *fitness* médio por instância, as diferenças nos desempenhos dos algoritmos se tornam imperceptíveis em dimensões maiores. Nas instâncias $D = 141, 142, 148 \text{ e } 149$, por exemplo, a mediana do Algoritmo 1 se encontra inferior a mediana do Algoritmo 2. Em contrapartida, nas instâncias $D = 143 \text{ e } 144$, se observa o inverso. Por fim, não se pode concluir nada a respeito dos segundos quartis nas instâncias $D = 145 \text{ e } 146$. Essa incerteza acerca das comparações entre as duas configurações propostas do algoritmo de Evolução Diferencial, torna ainda mais clara a necessidade da análise estatística para o estudo de caso que se segue.

```{r tail10, echo = FALSE, fig.height = 2.4, fig.width = 6, fig.align = 'center', fig.cap = 'Boxplot das dez últimas instâncias.', fig.pos = 'H'}
# União dos desempenhos dos dois algoritmos nas dez últimas instâncias
merged <- rbind(tail(head, 33*10), tail(tail, 33*10))

# Boxplot
p <- ggplot(data = merged, aes(x=variable, y=value)) + geom_boxplot(aes(fill=label))
p + labs(x = "Dimensão", y = "Fitness") +
    scale_fill_manual(name = "Algoritmo", values = c("#56B4E9", "#CD5C5C"))
```

# Potência do Teste

Tendo em vista que a análise estatística do presente trabalho será realizada sobre a experimentação completa, isto é, as 149 instâncias do problema ao invés apenas da quantidade mínima calculada, a potência obtida no teste sofrerá uma mudança e, portanto, deve ser calculada para o novo tamanho amostral. Dado que o tamanho de efeito não é afetado pelo tamanho da amostra ($d^{*} = 0,5$), tem-se a seguinte relação entre potência do teste e número de instâncias para um nível de significância constante e igual a $\alpha = 0,05$.

```{r, results = 'hide', echo = FALSE}
# Calcula relação entre potência e tamanho amostral
interval <- 2:150
power_values <- c()
for (i in interval) {
  out <- calc_instances(ncomparisons = 1, 
                       d = 0.5, 
                       ninstances = i, 
                       sig.level = 0.05, 
                       alternative.side = "one.sided", 
                       power.target = "mean")
  power_values[i-1] <- out$power
}
df <- data.frame(x = interval, y = power_values)
```

```{r, echo = FALSE, fig.height = 2.2, fig.width = 3, fig.align = 'center', fig.cap = 'Relação entre o poder do teste e o número de instâncias.', fig.pos = 'H'}
ggplot(data = df, aes(x = x, y = y, group = 1)) +
  xlab('Número de instâncias') + 
  ylab('Potência do teste') + 
  geom_line(linetype = 'dashed')+
  geom_point()
```

Aumentar o número de instâncias torna o teste de hipótese mais sensível, isto é, mais provável de rejeitar a hipótese nula quando ela é, de fato, falsa. Em outras palavras, a probabilidade de cometer um erro do tipo II ($\beta$) diminui à medida que o tamanho amostral aumenta. Como a potência do teste é o complemento do erro do tipo II ($\pi = 1-\beta$), então consequentemente ela aumenta. Desse modo, tem-se uma potência de $99,99\%$ ao se considerar 149 instâncias do problema com os demais parâmetros experimentais fixos.

```{r}
out <- calc_instances(ncomparisons = 1, 
                       d = 0.5, 
                       ninstances = 149, 
                       sig.level = 0.05, 
                       alternative.side = "one.sided", 
                       power.target = "mean")
cat('Potência alcançada:', out$power)
```

# Validação de Premissas

```{r, results = 'hide', echo = FALSE}
# Agregando desempenho médio dos algoritmos em todas as instâncias
aggdata <- data.frame(rbind(muAlgo1, muAlgo2))
# Renomeando colunas
colnames(aggdata) <- as.character(2:150)
# Renomeando linhas
rownames(aggdata) <- c('1', '2')
# Manipulando formato
aggdata <- melt(t(aggdata), id.vars = NULL)
print(aggdata)
colnames(aggdata) <- c('Instancia_Grupo', 'Algoritmo', 'f')
# Transformando variáveis categóricas em fatores
for (i in 1:2) aggdata[, i] <- as.factor(aggdata[, i])
```

Uma possibilidade de conduzir a análise deste experimento é por meio do Planejamento por Blocagem Completamente Randomizado, do inglês *Randomized Complete Block Design* (RCBD). Este método, também conhecido como Blocagem (*Blocking*), permite isolar os fatores indesejados, conhecidos e desconhecidos, e consequentemente conduzir a uma análise comparativa mais fidedígna, uma vez que o poder estatístico é impulsionado ao excluir a variabilidade entre as repetições dos resíduos \cite{Campelo2018-LNDoE}. Neste contexto, embora a aplicação do algoritmo em diferentes dimensões possa causar um efeito indesejado, a separação das instâncias por blocos exclui o efeito da instância e permite o estudo da média dos algoritmos.

Este experimento assume que há uma observação por bloco, blocos independentes e independência da aleatorização dentro do bloco \cite{Campelo2018-LNDoE}. Dessa forma, cada uma das dimensões dentro do intervalo proposto foi considerada uma instância do problema, e por isso, a média de cada uma das 33 execuções por instância é uma observação do bloco referente à respectiva instância, e somente a esta instância.

Para reduzir possíveis viéses causados por outliers, e melhorar o ajuste dos dados, facilitando a análise destes, foi aplicado uma transformação logarítmica sobre a variável de saída (*fitness* médio). 

```{r}
model <- aov(formula = log(f)~Algoritmo+Instancia_Grupo, data = aggdata)
```

Assim como no Anova, as premissas deste teste são normalidade dos resíduos, homocedasticidade e independência amostral. Tendo isso em vista, e com intuito de garantir a normalidade dos dados, cada uma das instâncias foi executada $33\times$, pois assim pode-se evocar o Teorema do Limite Central. Ainda sim, é importante notar que graficamente, os resíduos se ajustam bem à reta do QQ-plot, com a presença de somente dois outliers,  referentes à dimensão igual a 2 de cada um dos algoritmos. Quanto à homocedasticidade, observa-se que os resíduos comportam-se de maneira semelhante, com variabilidade constante, independente do algoritmo. A fim de garantir a independência das amostras, todas as execuções ocorreram de forma independente na mesma máquina, conforme relatado na coleta de dados. Diante da validação das premissas, é possível prosseguir para a análise estatística inicialmente proposta.

```{r, results = 'hide', echo = FALSE}
shapiro.test(model$residuals)
```

```{r qqplot, echo = FALSE, fig.height = 2, fig.width = 3, fig.align = 'center', fig.cap = 'QQ-Plot.', fig.pos = 'H'}
qqplot <- ggplot(data = as.data.frame(model$residuals), mapping = aes(sample = model$residuals))
qqplot + geom_qq_line() + geom_qq() + scale_y_continuous(name = 'Quantis da Amostra') + 
         scale_x_continuous(name = 'Quantis Teóricos Normais')
```

```{r, echo = FALSE, fig.height = 6, fig.width = 7, fig.pos = "H", fig.align = "center"}
# par(mfrow = c(2, 2))
# plot(model, pch = 20, las = 1)
```

# Análise Estatística

No R é possível ajustar um modelo de análise de variância (ANOVA) por meio da função `aov` \cite{AOV}, passando como parâmetro tanto o fator experimental a ser analisado (os algoritmos), como o fator a ser bloqueado, que neste contexto são as instâncias. Pode-se verificar que o somatório dos quadrados das instâncias foi muito superior ao observado nos algoritmos. Com isso, percebe-se que a blocagem deste elemento mostra-se fundamental para análise correta dos dados. Observa-se ainda que tanto em relação aos algoritmos, como se tratando das instâncias, os $p$ valores foram substancialmente pequenos, $p = 0,00061$ e $p = 2 \times 10 ^{-16}$, respectivamente, o que permite inferir a rejeição da hipótese nula ao nível de confiança de $95\%$. Assim, é possível relatar que as diferenças observadas nas médias dos algoritmos de fato existem.

Esta função também tem como saída o coeficiente de determinação, também conhecido como $R^{2}$. Trata-se de uma medida de ajuste que determina o quanto a variância dos dados pode ser explicada pelo modelo proposto. O $R^{2}$ pode assumir valores entre 0 e 1, e quanto mais próximo de 1, mais o modelo é uma boa representação das diferenças observadas. Neste contexto, $R^{2}$ resultou em $0,86$ que evidencia que as conclusões obtidas definem bem o experimento proposto.


```{r}
summary(model)
summary.lm(model)$r.squared
```

```{r}
# Relative blocking efficiency (E)
df <- as.data.frame(summary(model)[[1]])
MSblocks <- df["Instancia_Grupo","Mean Sq"]
MSe <- df["Residuals","Mean Sq"]
a <- length(unique(aggdata$Algoritmo))
b <- length(unique(aggdata$Instancia_Grupo))
E <- ((b - 1) * MSblocks + b * (a - 1) * MSe) / ((a * b - 1) * MSe)
cat('Relative blocking efficiency (E):', E)
```
Uma vez identificado através da análise das variâncias (ANOVA) que existe uma diferença estatística entre as medias das soluções dos algoritmos, porem não clara de qual algoritmo tem melhor desempenho perante o outro, deve realizar testes comparativos para tal.

Quando se deseja realizar múltiplas comparações, em geral tem dois métodos a ser utilizado, comparação todos contra todos e todos contra um. Todos contra um é utilizado quando se tem um "controle" e deseja-se comparar os demais tratamentos, algoritmos, soluções, com o "controle". No caso deste experimento, por se ter apenas dois algoritmos, o teste de comparação de todos contra um se resume a comparação de um com o outro. O teste de Dunnett neste caso foi utilizado por se ter maior sensibilidade \cite{Campelo2018-LNDoE}, atuando de forma semelhante ao $t-test$. Após apresentação do intervalo das diferenças das medias em escala logarítmica, pode-se concluir com 95% de confiança que o Algoritmo  2 apresenta melhor solução que o Algoritmo 1, devido ao intervalo está totalmente contido em valores negativos, $Algoritmo 1 - Algoritmo 2$.

```{r, echo = FALSE, results = 'hide', fig.height = 2.8, fig.width = 3, fig.align = 'center', fig.cap = 'Intervalo de confiança do teste de Dunnet ao nível de confiança de 95\\%.', fig.pos = 'H'}
duntest <- glht(model, linfct = mcp(Algoritmo = "Dunnett"))
duntestCI <- confint(duntest)
par(mar = c(5, 1, 1, 1), las = 1)
plot(duntestCI, xlab = "Diferença média (escala logarítmica)", main = "")
```

Como já mencionado anteriormente, aquele algoritmo que apresenta menor valor fitness apresenta uma melhor solução com relação ao outro. Já mencionado também que no caso deste experimento, nas dimensões extremas, ou seja, menores e maiores dimensões, os dois algoritmos não apresentam diferenças significativas graficamente. Dessa forma, foi destacado graficamente o intervalo das dimensões onde os algoritmos apresentavam diferenças na solução $D \in [20, 30]$, reforçando então o resultado do teste de Dunnet onde com $95%$ de confiança, devido ao intervalo das diferenças das medias (escala logarítmica), o $Algoritmo 2$ tem melhor solução que o $Algoritmo 1$.

```{r middle10, echo = FALSE, fig.height = 2.4, fig.width = 6, fig.align = 'center', fig.cap = 'Boxplot das instâncias contidas no domínio $20 \\leq D \\leq 30$.', fig.pos = 'H'}
# União dos desempenhos dos dois algoritmos nas dez instâncias do intervalo [20,30]
merged <- rbind(subset(head, as.numeric(variable) >= 19 & as.numeric(variable) < 30),
                subset(tail, as.numeric(variable) >= 19 & as.numeric(variable) < 30))

# Boxplot
p <- ggplot(data = merged, aes(x=variable, y=value)) + geom_boxplot(aes(fill=label))
p + labs(x = "Dimensão", y = "Fitness") +
    scale_fill_manual(name = "Algoritmo", values = c("#56B4E9", "#CD5C5C"))
```


# Conclusões
Sendo o maior desafio do experimento coletar e tabular os dados, isto porque devido a ordem de grandeza das dimensões do problema, é necessário uma maquina com bom poder de processamento e ou usar diferentes maquinas para rodar as soluções randomicamente em paralelo. Neste experimento, este passo foi superado ao utilizar uma maquina robusta, citada durante o estudo, onde foi possível resolver todas as instancias em tempo hábil, mesmo que o numero mínimo calculado foi de 27 instancias.
Ao comparar dois algoritmos, espera-se que um tenha melhor solução que o outro. A principio, graficamente, a diferença era pouco aparente, de qualquer forma, o estudo estatístico faz-se necessário entre outras aplicações, definir com certo nível de confiança, se esta diferença existe e qual tem melhor desempenho.
Sendo assim, após os estudos aqui mostrados, conclui-se que com $95%$ de confiança que existe diferença entre as soluções dos algoritmos e que também com $95%$ de confiança que o $Algoritmo 2$ apresenta melhor solução perante o $Algoritmo 1$.


## Discussão de Melhorias
Para melhor evidenciar o desempenho de um determinado algoritmo perante outros, seria valido ter mais algoritmos para se comparar. Claro depende da aplicação. Um ponto de atenção neste caso é que com mais algoritmos, o tratamento dos dados e os estudos estatísticos requereriam maiores níveis energia para chegar ao resultado.
Como neste experimento foi desempenhado a coleta dos dados em uma única maquina, não há como propor melhoria neste sentido, em outras palavras, caso o experimento tenha tido usado mais de uma maquina para executar as soluções, mesmo que usado instâncias aleatórias para retirar o efeito das diferenças das maquinas, executar em uma so maquina, neste caso, seria uma melhoria.
Outro fator de analise é com relação ao numero de instancias empregado. Para efeitos estatísticos, um numero mínimo de instancias pode ser considerado para um certo nível de potencia do teste. Neste experimento foi utilizado todas as instâncias, em outras palavras, para testes em que se utilizaram apenas o numero mínimo de instâncias, ao utilizar mais instâncias para o estudo estatístico, a potencia do teste torna-se maior.


## Atividades Desempenhadas



\renewcommand\refname{Referências}
\bibliographystyle{plain}
\bibliography{ref}